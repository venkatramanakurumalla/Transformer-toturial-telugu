‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç‡∞∏‡±ç ‡∞®‡±Å 7 ‡∞∞‡±ã‡∞ú‡±Å‡∞≤‡∞≤‡±ã ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞Ç
(‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞∏‡±ç‡∞™‡±Ä‡∞°‡±ç ‡∞µ‡±á ‡∞ó‡±à‡∞°‡±ç)

==========================================

üöÄ "‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç‡∞∏‡±ç ‡∞á‡∞®‡±ç 7 ‡∞°‡±á‡∞∏‡±ç" ‚Äì ‡∞∏‡±ç‡∞™‡±Ä‡∞°‡±ç ‡∞µ‡±á ‡∞™‡±ç‡∞≤‡∞æ‡∞®‡±ç

‡∞à ‡∞™‡±ç‡∞≤‡∞æ‡∞®‡±ç ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‡∞∏‡±ç ‡∞®‡±Å ‡∞§‡±ç‡∞µ‡∞∞‡∞ó‡∞æ ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±Å‡∞®‡∞ø, ‡∞™‡±ç‡∞∞‡∞æ‡∞ï‡±ç‡∞ü‡∞ø‡∞ï‡∞≤‡±ç ‡∞ó‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡∞æ‡∞∞‡±Å.

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 1: ‡∞¨‡±á‡∞∏‡∞ø‡∞ï‡±ç‡∞∏‡±ç & ‡∞Ö‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç ‡∞Æ‡±Ü‡∞ï‡∞æ‡∞®‡∞ø‡∞ú‡∞Ç

‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞Ç: ‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞Ç‡∞¶‡±ã ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø.

üìå ‡∞ö‡∞¶‡∞µ‡∞Ç‡∞°‡∞ø:
- https://jalammar.github.io/illustrated-transformer/
- RNN/LSTM ‡∞≤ ‡∞≤‡±ã‡∞™‡∞æ‡∞≤‡±Å
- Attention ‡∞Ö‡∞Ç‡∞ü‡±á ‡∞è‡∞Ç‡∞ü‡∞ø?

üß† ‡∞ï‡±Ä ‡∞™‡∞æ‡∞Ø‡∞ø‡∞Ç‡∞ü‡±ç‡∞∏‡±ç:
- "Attention is All You Need" ‡∞™‡±á‡∞™‡∞∞‡±ç ‡∞≤‡±ã‡∞®‡∞ø ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞æ‡∞Ç‡∞∂‡∞Ç
- ‡∞∏‡±Ü‡∞≤‡±ç‡∞´‡±ç ‡∞Ö‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç ‡∞Ö‡∞Ç‡∞ü‡±á ‡∞è‡∞Ç‡∞ü‡∞ø?

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 2-3 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 2: ‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç ‡∞Ü‡∞∞‡±ç‡∞ï‡∞ø‡∞ü‡±Ü‡∞ï‡±ç‡∞ö‡∞∞‡±ç

‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞Ç: ‡∞é‡∞®‡±ç‡∞ï‡±ã‡∞°‡∞∞‡±ç, ‡∞°‡±Ä‡∞ï‡±ã‡∞°‡∞∞‡±ç, ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø.

üìå ‡∞ö‡∞¶‡∞µ‡∞Ç‡∞°‡∞ø:
- ‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞∞‡±ç ‡∞¨‡±ç‡∞≤‡∞æ‡∞ï‡±ç ‡∞°‡∞Ø‡∞æ‡∞ó‡±ç‡∞∞‡∞Æ‡±ç
- ‡∞™‡±ä‡∞ú‡∞ø‡∞∑‡∞®‡∞≤‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç?
- ‡∞Æ‡∞≤‡±ç‡∞ü‡±Ä-‡∞π‡±Ü‡∞°‡±ç ‡∞Ö‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç

üéØ ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø:
- ‡∞é‡∞®‡±ç‡∞ï‡±ã‡∞°‡∞∞‡±ç ‡∞¨‡±ç‡∞≤‡∞æ‡∞ï‡±ç ‡∞≤‡±ã‡∞™‡∞≤ ‡∞è‡∞Ç ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?
- ‡∞°‡±Ä‡∞ï‡±ã‡∞°‡∞∞‡±ç ‡∞Æ‡∞æ‡∞∏‡±ç‡∞ï‡±ç ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞∞‡±Å?

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 2 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 3: ‡∞π‡∞ó‡±ç‡∞ó‡∞ø‡∞Ç‡∞ó‡±ç ‡∞´‡±á‡∞∏‡±ç ‡∞§‡±ã ‡∞™‡±ç‡∞∞‡∞æ‡∞ï‡±ç‡∞ü‡∞ø‡∞ï‡∞≤‡±ç

‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞Ç: ‡∞∞‡±Ü‡∞°‡±Ä ‡∞Æ‡±á‡∞°‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‡∞∏‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø.

üêç ‡∞ï‡±ã‡∞°‡±ç:
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love AI in Telugu!")
print(result)

üìå ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø:
- pipeline() ‡∞é‡∞≤‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø
- ‡∞ü‡∞æ‡∞∏‡±ç‡∞ï‡±Å‡∞≤‡±Å: Translation, Summarization, Q&A

üîó ‡∞µ‡∞®‡∞∞‡±Å: https://huggingface.co/docs/transformers/quicktour

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 2-3 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 4: BERT vs GPT

üìå BERT: Bidirectional ‚Üí ‡∞á‡∞®‡±ç‡∞™‡±Å‡∞ü‡±ç ‡∞Æ‡±ä‡∞§‡±ç‡∞§‡∞Ç ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø  
üìå GPT: Autoregressive ‚Üí ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞ú‡∞®‡∞∞‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø

üß™ ‡∞ï‡±ã‡∞°‡±ç:
# BERT - Fill Mask
from transformers import pipeline
fill_mask = pipeline("fill-mask", model="bert-base-uncased")
print(fill_mask("Telugu is a beautiful [MASK] language."))

# GPT-2 - Text Generation
generator = pipeline("text-generation", model="gpt2")
print(generator("Artificial Intelligence in Telugu is", max_length=50))

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 2 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 5: ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç & ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç

üêç ‡∞ï‡±ã‡∞°‡±ç:
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "Transformers are amazing!"
tokens = tokenizer(text, return_tensors="pt")
print(tokens)
print(tokenizer.convert_ids_to_tokens(tokens["input_ids"][0]))

üß† ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø:
- [CLS], [SEP], [MASK] ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç‡∞∏‡±ç
- Attention mask ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å?

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 2 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 6: ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç (‡∞ö‡∞ø‡∞®‡±ç‡∞® ‡∞™‡±ç‡∞∞‡∞æ‡∞ú‡±Ü‡∞ï‡±ç‡∞ü‡±ç)

üéØ ‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞Ç: BERT ‡∞®‡∞ø ‡∞∏‡±Ü‡∞Ç‡∞ü‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡∞ø‡∞´‡∞ø‡∞ï‡±á‡∞∑‡∞®‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø.

üõ†Ô∏è ‡∞ü‡±Ç‡∞≤‡±ç‡∞∏‡±ç: Google Colab, Hugging Face Trainer

üì¶ ‡∞ï‡±ã‡∞°‡±ç ‡∞∏‡±ç‡∞ï‡±Ü‡∞≤‡∞ø‡∞ü‡∞®‡±ç:
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

dataset = load_dataset("imdb")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_train_batch_size=8,
    num_train_epochs=1,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"].select(range(1000)),
)

trainer.train()

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 3-4 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

‚úÖ ‡∞¶‡∞ø‡∞®‡∞Ç 7: ‡∞™‡±ç‡∞∞‡∞æ‡∞ú‡±Ü‡∞ï‡±ç‡∞ü‡±ç

üéØ ‡∞ê‡∞°‡∞ø‡∞Ø‡∞æ‡∞∏‡±ç:
1. ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å-‡∞á‡∞Ç‡∞ó‡±ç‡∞≤‡±Ä‡∞∑‡±ç ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç
2. ‡∞∏‡∞ø‡∞Æ‡±ç‡∞™‡±Å‡∞≤‡±ç ‡∞ö‡∞æ‡∞ü‡±ç ‡∞¨‡∞æ‡∞ü‡±ç
3. ‡∞∏‡±Ü‡∞Ç‡∞ü‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞Ö‡∞®‡∞æ‡∞≤‡∞ø‡∞∏‡∞ø‡∞∏‡±ç for Telugu

üåç ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£:
from transformers import pipeline
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-te-en")
print(translator("‡∞®‡±á‡∞®‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞∏‡∞Ç‡∞§‡±ã‡∞∑‡∞Ç‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å"))

‚è±Ô∏è ‡∞∏‡∞Æ‡∞Ø‡∞Ç: 3 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å

==========================================

üéØ ‡∞∏‡±ç‡∞™‡±Ä‡∞°‡±ç ‡∞µ‡±á ‡∞ü‡∞ø‡∞™‡±ç‡∞∏‡±ç

| ‡∞ü‡∞ø‡∞™‡±ç | ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£ |
|------|--------|
| 1. ‡∞ï‡±á‡∞µ‡∞≤‡∞Ç ‡∞•‡∞ø‡∞Ø‡∞∞‡±Ä ‡∞ï‡∞æ‡∞¶‡±Å | ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞∞‡±ã‡∞ú‡±Å ‡∞ï‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø |
| 2. Hugging Face ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø | 1000+ ‡∞∞‡±Ü‡∞°‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‡∞∏‡±ç |
| 3. Colab ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø | ‡∞´‡±ç‡∞∞‡±Ä GPU |
| 4. ‡∞™‡±á‡∞™‡∞∞‡±ç ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø‡∞ï‡∞ø ‡∞ö‡∞¶‡∞µ‡∞Ç‡∞°‡∞ø | ‡∞Æ‡±ä‡∞¶‡∞ü ‡∞™‡±ç‡∞∞‡∞æ‡∞ï‡±ç‡∞ü‡∞ø‡∞∏‡±ç |
| 5. 80/20 ‡∞∞‡±Ç‡∞≤‡±ç | 20% ‡∞ï‡∞æ‡∞®‡±ç‡∞∏‡±Ü‡∞™‡±ç‡∞ü‡±ç‡∞∏‡±ç ‡∞§‡±ã 80% ‡∞™‡∞®‡∞ø |

==========================================

üìö ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞™‡∞°‡±á ‡∞µ‡∞®‡∞∞‡±Å‡∞≤‡±Å

1. Hugging Face Course ‚Üí https://huggingface.co/course
2. The Illustrated Transformer ‚Üí https://jalammar.github.io/illustrated-transformer/
3. YouTube: "Transformers in Telugu" ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
4. Google Colab ‚Üí https://colab.research.google.com

==========================================

#AILearningInTelugu #TransformersIn7Days #FastTrackAI #NLPInTelugu üöÄ
